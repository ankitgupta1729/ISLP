{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063f56d4-8eed-41f2-9b58-63d7c22c06bb",
   "metadata": {},
   "source": [
    "## Lab: Cross-Validation and the Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb4daee-fea6-4ebc-b7c6-102211435d23",
   "metadata": {},
   "source": [
    "In this lab, we explore the resampling techniques covered in this chapter. Some of the commands in this lab may take a while to run on your\n",
    "computer.   \n",
    "    \n",
    "We again begin by placing most of our imports at this top level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e612fb-25cb-4ae0-a4f2-fba816402b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,summarize,poly)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0713837-7d8b-4859-b5df-71c3fb096a91",
   "metadata": {},
   "source": [
    "There are several new imports needed for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c42752c-dd16-4d9f-9192-86343dfcdce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from sklearn.model_selection import (cross_validate,KFold,ShuffleSplit)\n",
    "from sklearn.base import clone\n",
    "from ISLP.models import sklearn_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49867e6f-545c-490d-9cfc-d88f3cff54f8",
   "metadata": {},
   "source": [
    "## 5.3.1 The Validation Set Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e12fa-d25a-478a-9ca9-1f99a3489891",
   "metadata": {},
   "source": [
    "We explore the use of the validation set approach in order to estimate the\n",
    "test error rates that result from ftting various linear models on the Auto\n",
    "data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f7f4d1-0b2f-4d4d-a336-aab4105f0660",
   "metadata": {},
   "source": [
    "We use the function train_test_split() to split the data into training and validation sets. As there are 392 observations, we split into two equal\n",
    "sets of size 196 using the argument test_size=196. It is generally a good\n",
    "idea to set a random seed when performing operations like this that contain\n",
    "an element of randomness, so that the results obtained can be reproduced\n",
    "precisely at a later time. We set the random seed of the splitter with the\n",
    "argument random_state=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19a84077-9b99-4d77-8fd2-8a6a2ee14d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto = load_data('Auto')\n",
    "Auto_train, Auto_valid = train_test_split(Auto,test_size=196,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1ea177-b01e-47dc-955c-575fdeeefb94",
   "metadata": {},
   "source": [
    "Now we can ft a linear regression using only the observations corresponding to the training set Auto_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2091f79-45e2-4a8f-93c3-42c041f6a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_mm = MS(['horsepower'])\n",
    "X_train = hp_mm.fit_transform(Auto_train)\n",
    "y_train = Auto_train['mpg']\n",
    "model = sm.OLS(y_train, X_train)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3aeada-a41b-4b04-bf06-02d13d2ddeb4",
   "metadata": {},
   "source": [
    "We now use the predict() method of results evaluated on the model matrix for this model created using the validation data set. We also calculate\n",
    "the validation MSE of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aed52cf-e1da-4ec1-82a9-b16633e3ff52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.61661706966988"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid = hp_mm.transform(Auto_valid)\n",
    "y_valid = Auto_valid['mpg']\n",
    "valid_pred = results.predict(X_valid)\n",
    "np.mean((y_valid - valid_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc8cb9b-0b1d-4725-a436-3e75a56fcc87",
   "metadata": {},
   "source": [
    "Hence our estimate for the validation MSE of the linear regression ft is\n",
    "23.62."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8d67d-b280-468d-b6be-3baab7fb8f18",
   "metadata": {},
   "source": [
    "We can also estimate the validation error for higher-degree polynomial\n",
    "regressions. We frst provide a function evalMSE() that takes a model string\n",
    "as well as a training and test set and returns the MSE on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d9762c9-5bce-41d5-a0d0-9b75466057cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalMSE(terms,response,train,test):\n",
    "    mm = MS(terms)\n",
    "    X_train = mm.fit_transform(train)\n",
    "    y_train = train[response]\n",
    "    X_test = mm.transform(test)\n",
    "    y_test = test[response]\n",
    "    results = sm.OLS(y_train, X_train).fit()\n",
    "    test_pred = results.predict(X_test)\n",
    "    return np.mean((y_test - test_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104506aa-1e49-40ae-87b8-d18b0eca1f8d",
   "metadata": {},
   "source": [
    "Let’s use this function to estimate the validation MSE using linear,\n",
    "quadratic and cubic fts. We use the enumerate() function here, which gives both the values and indices of objects as one iterates over a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92733cfc-3bdc-4735-95e8-f35f605bda59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.61661707, 18.76303135, 18.79694163])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE = np.zeros(3)\n",
    "for idx, degree in enumerate(range(1, 4)):\n",
    "    MSE[idx] = evalMSE([poly('horsepower', degree)],'mpg',Auto_train,Auto_valid)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79dd489-6c02-4031-9ed4-85c52f22640a",
   "metadata": {},
   "source": [
    "These error rates are 23.62, 18.76, and 18.80, respectively. If we choose a\n",
    "diferent training/validation split instead, then we can expect somewhat\n",
    "diferent errors on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc9097b1-8ea3-4cfc-a43e-5950ebf0f31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.75540796, 16.94510676, 16.97437833])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Auto_train, Auto_valid = train_test_split(Auto,test_size=196,random_state=3)\n",
    "MSE = np.zeros(3)\n",
    "for idx, degree in enumerate(range(1, 4)):\n",
    "    MSE[idx] = evalMSE([poly('horsepower', degree)],'mpg',Auto_train,Auto_valid)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddb812d-530d-49fc-ad0f-6a52c01b6808",
   "metadata": {},
   "source": [
    "Using this split of the observations into a training set and a validation\n",
    "set, we fnd that the validation set error rates for the models with linear,\n",
    "quadratic, and cubic terms are 20.76, 16.95, and 16.97, respectively.    \n",
    "    \n",
    "These results are consistent with our previous fndings: a model that\n",
    "predicts mpg using a quadratic function of horsepower performs better than\n",
    "a model that involves only a linear function of horsepower, and there is no\n",
    "evidence of an improvement in using a cubic function of horsepower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e88d2e-1b46-4b96-8f26-dedbf3e3aae7",
   "metadata": {},
   "source": [
    "## 5.3.2 Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c045b164-4912-4c38-a2da-3c6e77daae4b",
   "metadata": {},
   "source": [
    "In theory, the cross-validation estimate can be computed for any generalized linear model. In practice, however, the simplest way to cross-validate\n",
    "in Python is to use sklearn, which has a diferent interface or API than\n",
    "statsmodels, the code we have been using to ft GLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a8b4ca-e63c-459d-8cb5-c37ba6450f0e",
   "metadata": {},
   "source": [
    "This is a problem which often confronts data scientists: “I have a function\n",
    "to do task A, and need to feed it into something that performs task B, so\n",
    "that I can compute B(A(D)), where D is my data.” When A and B don’t\n",
    "naturally speak to each other, this requires the use of a wrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3276a232-c132-4c10-b78e-4ed2470e8f63",
   "metadata": {},
   "source": [
    "In the ISLP wrapper package, we provide a wrapper, sklearn_sm(), that enables us to easily use the cross-validation tools of sklearn with models ft by statsmodels.   \n",
    "   \n",
    "The class sklearn_sm() has as its frst argument a model from statsmodels.\n",
    "It can take two additional optional arguments: model_str which can be used\n",
    "to specify a formula, and model_args which should be a dictionary of additional arguments used when ftting the model. For example, to ft a logistic\n",
    "regression model we have to specify a family argument. This is passed as\n",
    "model_args={'family':sm.families.Binomial()}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf5f3f-3580-4121-83cb-670b486d50df",
   "metadata": {},
   "source": [
    "Here is our wrapper in action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e789ca8-00fe-4ea9-96df-0e4dfd89b7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.231513517929212"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_model = sklearn_sm(sm.OLS,MS(['horsepower']))\n",
    "X, Y = Auto.drop(columns=['mpg']), Auto['mpg']\n",
    "cv_results = cross_validate(hp_model,X,Y,cv=Auto.shape[0])\n",
    "cv_err = np.mean(cv_results['test_score'])\n",
    "cv_err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c48c185-1961-4489-af3a-a8bc10c2c27f",
   "metadata": {},
   "source": [
    "The arguments to cross_validate() are as follows: an object with the appropriate fit(), predict(), and score() methods, an array of features X and\n",
    "a response Y. We also included an additional argument cv to cross_validate();\n",
    "specifying an integer K results in K-fold cross-validation. We have provided\n",
    "a value corresponding to the total number of observations, which results\n",
    "in leave-one-out cross-validation (LOOCV). The cross_validate() function produces a dictionary with several components; we simply want the\n",
    "cross-validated test score here (MSE), which is estimated to be 24.23."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953b14c9-0d44-40ce-a4c8-a86cf024c13b",
   "metadata": {},
   "source": [
    "We can repeat this procedure for increasingly complex polynomial fts.\n",
    "To automate the process, we again use a for loop which iteratively fts\n",
    "polynomial regressions of degree 1 to 5, computes the associated crossvalidation error, and stores it in the ith element of the vector cv_error.\n",
    "The variable d in the for loop corresponds to the degree of the polynomial.\n",
    "We begin by initializing the vector. This command may take a couple of\n",
    "seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f60386d-5ec2-404b-8059-fc8d115f05fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.23151352, 19.24821312, 19.33498406, 19.42443031, 19.03320428])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "H = np.array(Auto['horsepower'])\n",
    "M = sklearn_sm(sm.OLS)\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M,X,Y,cv=Auto.shape[0])\n",
    "    cv_error[i] = np.mean(M_CV['test_score'])\n",
    "cv_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525ad421-aa4c-4e74-a04d-66e8ecc1b571",
   "metadata": {},
   "source": [
    "As in Figure 5.4, we see a sharp drop in the estimated test MSE between\n",
    "the linear and quadratic fts, but then no clear improvement from using\n",
    "higher-degree polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd47c6-0839-4941-9673-cbfc89b96d7a",
   "metadata": {},
   "source": [
    "Above we introduced the outer() method of the np.power() function. .outer()\n",
    "np.power() The outer() method is applied to an operation that has two arguments,\n",
    "such as add(), min(), or power(). It has two arrays as arguments, and then\n",
    "forms a larger array where the operation is applied to each pair of elements\n",
    "of the two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cabc7030-c961-4a8a-a9ac-b69a6faaf694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  7],\n",
       "       [ 7,  9],\n",
       "       [11, 13]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([3, 5, 9])\n",
    "B = np.array([2, 4])\n",
    "np.add.outer(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b6d159-dc6b-437a-a24d-a6ffca8cc140",
   "metadata": {},
   "source": [
    "In the CV example above, we used K = n, but of course we can also use\n",
    "K<n. The code is very similar to the above (and is signifcantly faster).\n",
    "Here we use KFold() to partition the data into K = 10 random groups. We \n",
    "use random_state to set a random seed and initialize a vector cv_error in\n",
    "which we will store the CV errors corresponding to the polynomial fts of\n",
    "degrees one to fve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c5279bf-c7bd-461f-a972-f03d41345ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.20766449, 19.18533142, 19.27626666, 19.47848404, 19.13722016])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "cv = KFold(n_splits=10,shuffle=True,random_state=0) # use same splits for each degree\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M,X,Y,cv=cv)\n",
    "    cv_error[i] = np.mean(M_CV['test_score'])\n",
    "cv_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8776f92a-628b-4d8c-a9d5-cdab6db7ce80",
   "metadata": {},
   "source": [
    "Notice that the computation time is much shorter than that of LOOCV.\n",
    "(In principle, the computation time for LOOCV for a least squares linear\n",
    "model should be faster than for K-fold CV, due to the availability of the\n",
    "formula (5.2) for LOOCV; however, the generic cross_validate() function\n",
    "does not make use of this formula.) We still see little evidence that using\n",
    "cubic or higher-degree polynomial terms leads to a lower test error than\n",
    "simply using a quadratic ft."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e150173b-cf0c-4057-9849-ea597ef212a5",
   "metadata": {},
   "source": [
    "The cross_validate() function is fexible and can take diferent splitting\n",
    "mechanisms as an argument. For instance, one can use the ShuffleSplit()  funtion to implement the validation set approach just as easily as K-fold\n",
    "cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "420783c2-b52b-4ba9-bcc7-a2e80dcba77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.61661707])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = ShuffleSplit(n_splits=1,test_size=196,random_state=0)\n",
    "results = cross_validate(hp_model,Auto.drop(['mpg'], axis=1),Auto['mpg'],cv=validation);\n",
    "results['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24e77478-332b-4e36-a996-3ce92f63030f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23.802232661034164, 1.4218450941091847)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = ShuffleSplit(n_splits=10,test_size=196,random_state=0)\n",
    "results = cross_validate(hp_model,Auto.drop(['mpg'], axis=1),Auto['mpg'],cv=validation)\n",
    "results['test_score'].mean(), results['test_score'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d3fa33-136d-4516-a3fa-fec81c2d9d16",
   "metadata": {},
   "source": [
    "Note that this standard deviation is not a valid estimate of the sampling variability of the mean test score or the individual scores, since the\n",
    "randomly-selected training samples overlap and hence introduce correlations. But it does give an idea of the Monte Carlo variation incurred by\n",
    "picking diferent random folds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0719fda5-c556-442d-bf88-12e57b921b10",
   "metadata": {},
   "source": [
    "## 5.3.3 The Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffb506e-7903-4e92-89fd-9d1dac061dfa",
   "metadata": {},
   "source": [
    "We illustrate the use of the bootstrap in the simple example of Section 5.2,\n",
    "as well as on an example involving estimating the accuracy of the linear\n",
    "regression model on the Auto data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdea7233-6a2e-4cde-8ca6-954346b186fd",
   "metadata": {},
   "source": [
    "## Estimating the Accuracy of a Statistic of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416aeb41-c932-43a6-a6d9-6c13ee5200a9",
   "metadata": {},
   "source": [
    "One of the great advantages of the bootstrap approach is that it can be\n",
    "applied in almost all situations. No complicated mathematical calculations\n",
    "are required. While there are several implementations of the bootstrap in\n",
    "Python, its use for estimating standard error is simple enough that we write\n",
    "our own function below for the case when our data is stored in a dataframe.   \n",
    "\n",
    "    \n",
    "To illustrate the bootstrap, we start with a simple example. The Portfolio\n",
    "data set in the ISLP package is described in Section 5.2. The goal is to estimate the sampling variance of the parameter α given in formula (5.7).\n",
    "We will create a function alpha_func(), which takes as input a dataframe D\n",
    "assumed to have columns X and Y, as well as a vector idx indicating which\n",
    "observations should be used to estimate α. The function then outputs the\n",
    "estimate for α based on the selected observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e741c3c0-641a-46de-9cc6-6d1f454bf33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Portfolio = load_data('Portfolio')\n",
    "def alpha_func(D, idx):\n",
    "    cov_ = np.cov(D[['X','Y']].loc[idx], rowvar=False)\n",
    "    return ((cov_[1,1] - cov_[0,1]) /(cov_[0,0]+cov_[1,1]-2*cov_[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f803c719-e113-45f6-856f-c7539b981121",
   "metadata": {},
   "source": [
    "This function returns an estimate for α based on applying the minimum\n",
    "variance formula (5.7) to the observations indexed by the argument idx. For\n",
    "instance, the following command estimates α using all 100 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31f2d74c-a057-4462-b572-00ecc78ec8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57583207459283"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_func(Portfolio, range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed98797a-c738-4acc-8341-4db44aa3ebcc",
   "metadata": {},
   "source": [
    "Next we randomly select 100 observations from range(100), with replacement. This is equivalent to constructing a new bootstrap data set and\n",
    "recomputing αˆ based on the new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cade2aa5-e4a6-40c2-926f-a827ec795e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6074452469619004"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "alpha_func(Portfolio,rng.choice(100,100,replace=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdaed2b-98da-4865-8f13-49763cf97371",
   "metadata": {},
   "source": [
    "This process can be generalized to create a simple function boot_SE() for\n",
    "computing the bootstrap standard error for arbitrary functions that take\n",
    "only a data frame as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "975efb6e-1649-4069-9330-8ee750d9cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_SE(func,D,n=None,B=1000,seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    first_, second_ = 0, 0\n",
    "    n = n or D.shape[0]\n",
    "    for _ in range(B):\n",
    "        idx = rng.choice(D.index,n,replace=True)\n",
    "        value = func(D, idx)\n",
    "        first_ += value\n",
    "        second_ += value**2\n",
    "    return np.sqrt(second_ / B - (first_ / B)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e8304c-e475-4d97-a11c-3642ea755ee4",
   "metadata": {},
   "source": [
    "Notice the use of _ as a loop variable in for _ in range(B). This is often\n",
    "used if the value of the counter is unimportant and simply makes sure the\n",
    "loop is executed B times.    \n",
    "        \n",
    "Let’s use our function to evaluate the accuracy of our estimate of α using\n",
    "B = 1,000 bootstrap replications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1b94fba-e004-45f3-9e3b-f26072c6f73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09118176521277699"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_SE = boot_SE(alpha_func,Portfolio,B=1000,seed=0)\n",
    "alpha_SE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d30065c-f945-4224-a410-90a3eb7cbfb9",
   "metadata": {},
   "source": [
    "The fnal output shows that the bootstrap estimate for SE(ˆα) is 0.0912."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaff374-787c-47a1-af96-30f34cda52d4",
   "metadata": {},
   "source": [
    "## Estimating the Accuracy of a Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef48d7-cfab-4d6d-97bc-108909c0bc9b",
   "metadata": {},
   "source": [
    "The bootstrap approach can be used to assess the variability of the coeffcient estimates and predictions from a statistical learning method. Here\n",
    "we use the bootstrap approach in order to assess the variability of the estimates for β0 and β1, the intercept and slope terms for the linear regression model that uses horsepower to predict mpg in the Auto data set. We\n",
    "will compare the estimates obtained using the bootstrap to those obtained\n",
    "using the formulas for SE(βˆ0) and SE(βˆ1) described in Section 3.1.2.   \n",
    "     \n",
    "To use our boot_SE() function, we must write a function (its frst argument) that takes a data frame D and indices idx as its only arguments. But\n",
    "here we want to bootstrap a specifc regression model, specifed by a model\n",
    "formula and data. We show how to do this in a few simple steps.\n",
    "\n",
    "      \n",
    "We start by writing a generic function boot_OLS() for bootstrapping a\n",
    "regression model that takes a formula to defne the corresponding regression. We use the clone() function to make a copy of the formula that can clone() be reft to the new dataframe. This means that any derived features such\n",
    "as those defned by poly() (which we will see shortly), will be re-ft on the\n",
    "resampled data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d22f7337-84f2-4984-a003-56e3297a2695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_OLS(model_matrix, response, D, idx):\n",
    "    D_ = D.loc[idx]\n",
    "    Y_ = D_[response]\n",
    "    X_ = clone(model_matrix).fit_transform(D_)\n",
    "    return sm.OLS(Y_, X_).fit().params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4591b58-ad6d-4975-afd0-f8fb72a17806",
   "metadata": {},
   "source": [
    "This is not quite what is needed as the frst argument to boot_SE(). The frst\n",
    "two arguments which specify the model will not change in the bootstrap\n",
    "process, and we would like to freeze them. The function partial() from the functools module does precisely this: it takes a function as an argument,\n",
    "and freezes some of its arguments, starting from the left. We use it to freeze\n",
    "the frst two model-formula arguments of boot_OLS()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a142e8b-65c6-4d7f-b14a-f7e09a1ae681",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_func = partial(boot_OLS, MS(['horsepower']), 'mpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496e4223-f6b9-432d-9998-8a49f40a5e38",
   "metadata": {},
   "source": [
    "Typing hp_func? will show that it has two arguments D and idx — it is a\n",
    "version of boot_OLS() with the frst two arguments frozen — and hence is\n",
    "ideal as the frst argument for boot_SE(). \n",
    "                               \n",
    "The hp_func() function can now be used in order to create bootstrap\n",
    "estimates for the intercept and slope terms by randomly sampling from\n",
    "among the observations with replacement. We frst demonstrate its utility\n",
    "on 10 bootstrap samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a929bd2c-6a7b-443e-98a7-24189b086988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39.88064456, -0.1567849 ],\n",
       "       [38.73298691, -0.14699495],\n",
       "       [38.31734657, -0.14442683],\n",
       "       [39.91446826, -0.15782234],\n",
       "       [39.43349349, -0.15072702],\n",
       "       [40.36629857, -0.15912217],\n",
       "       [39.62334517, -0.15449117],\n",
       "       [39.0580588 , -0.14952908],\n",
       "       [38.66688437, -0.14521037],\n",
       "       [39.64280792, -0.15555698]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "np.array([hp_func(Auto,rng.choice(392,392,replace=True)) for _ in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a943705c-3244-441c-a1b1-49ec491f6d73",
   "metadata": {},
   "source": [
    "Next, we use the boot_SE() function to compute the standard errors of\n",
    "1,000 bootstrap estimates for the intercept and slope terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1f96bbc-635f-4ab7-bfc3-1fed0e4402c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept     0.848807\n",
       "horsepower    0.007352\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_se = boot_SE(hp_func,Auto,B=1000,seed=10)\n",
    "hp_se"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e29481-99ca-4e33-979e-4cfa5d9003ba",
   "metadata": {},
   "source": [
    "This indicates that the bootstrap estimate for SE(βˆ0) is 0.85, and that\n",
    "the bootstrap estimate for SE(βˆ1) is 0.0074. As discussed in Section 3.1.2,\n",
    "standard formulas can be used to compute the standard errors for the\n",
    "regression coefcients in a linear model. These can be obtained using the\n",
    "summarize() function from ISLP.sm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3d362d9-a78d-4a17-bba4-652ed7ffe969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept     0.717\n",
       "horsepower    0.006\n",
       "Name: std err, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_model.fit(Auto, Auto['mpg'])\n",
    "model_se = summarize(hp_model.results_)['std err']\n",
    "model_se"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba59e6cb-d3a3-4b86-afa8-2a6d37e6cce7",
   "metadata": {},
   "source": [
    "The standard error estimates for βˆ0 and βˆ1 obtained using the formulas\n",
    "from Section 3.1.2 are 0.717 for the intercept and 0.006 for the slope. Interestingly, these are somewhat diferent from the estimates obtained using\n",
    "the bootstrap. Does this indicate a problem with the bootstrap? In fact,\n",
    "it suggests the opposite. Recall that the standard formulas given in Equation 3.8 on page 75 rely on certain assumptions. For example, they depend\n",
    "on the unknown parameter σ2, the noise variance. We then estimate σ2\n",
    "using the RSS. Now although the formula for the standard errors do not\n",
    "rely on the linear model being correct, the estimate for σ2 does. We see in\n",
    "Figure 3.8 on page 99 that there is a non-linear relationship in the data,\n",
    "and so the residuals from a linear ft will be infated, and so will σˆ2. Secondly, the standard formulas assume (somewhat unrealistically) that the\n",
    "xi are fxed, and all the variability comes from the variation in the errors\n",
    "\"i. The bootstrap approach does not rely on any of these assumptions, and\n",
    "so it is likely giving a more accurate estimate of the standard errors of βˆ0\n",
    "and βˆ1 than the results from sm.OLS.   \n",
    "                                           \n",
    "Below we compute the bootstrap standard error estimates and the standard linear regression estimates that result from ftting the quadratic model\n",
    "to the data. Since this model provides a good ft to the data (Figure 3.8),\n",
    "there is now a better correspondence between the bootstrap estimates and\n",
    "the standard estimates of SE(βˆ0), SE(βˆ1) and SE(βˆ2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b708dd25-3d89-4c28-aca8-42e1b9ebaff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept                                  2.067840\n",
       "poly(horsepower, degree=2, raw=True)[0]    0.033019\n",
       "poly(horsepower, degree=2, raw=True)[1]    0.000120\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quad_model = MS([poly('horsepower', 2, raw=True)])\n",
    "quad_func = partial(boot_OLS,quad_model,'mpg')\n",
    "boot_SE(quad_func, Auto, B=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a83240-90cf-4876-945d-39e20210ab8b",
   "metadata": {},
   "source": [
    "We compare the results to the standard errors computed using sm.OLS()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7aee9fa-a8d7-4286-afb2-b9fdea0ab425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept                                  1.800\n",
       "poly(horsepower, degree=2, raw=True)[0]    0.031\n",
       "poly(horsepower, degree=2, raw=True)[1]    0.000\n",
       "Name: std err, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = sm.OLS(Auto['mpg'],quad_model.fit_transform(Auto))\n",
    "summarize(M.fit())['std err']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddf27b8-0a95-4a5e-a2ee-587136de0df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
